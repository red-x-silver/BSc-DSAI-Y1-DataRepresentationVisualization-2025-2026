<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
  <title>Data Representation Visualization</title>
  <!-- This whole presentation was made with Big: https://github.com/tmcw/big -->
  
  <link href="../big/big.css" rel="stylesheet" type="text/css" />
  <script src="../big/big.js"></script>
  <link href="../big/themes/lightWhite.css" rel="stylesheet" type="text/css" />

  <!-- Favicon link below, via https://emojitofavicon.com -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%2210 0 100 100%22><text y=%22.90em%22 font-size=%2290%22>ğŸ´</text></svg>"></link>
  
  <style>
    body {
      background-color: #EDDD6E;
      font-family: -apple-system, BlinkMacSystemFont, avenir next, avenir, helvetica neue, helvetica, Ubuntu, roboto, noto, segoe ui, arial, sans-serif;
      /* font-weight: 700;*/
      margin: 0;
      padding: 0;
      font-style: normal;
      font-size: 21px;
    }
  </style>
  
</head>
<body>
  <div>
    DRVğŸ‘
    <br />
    Lecture 08
    <br />
    Introduction to text data representation and visualization using Python
  </div>
    
  <div>Welcome ğŸ‘©â€ğŸ¤ğŸ§‘â€ğŸ¤ğŸ‘¨â€ğŸ¤</div>
  
  <div> By the end of this lecture, we'll have learnt about:
  
    <br /> - how to represent text data numerically
    <br /> - the pipeline for representing and visualizing text data: preprocess, tokenization, numerical representation for tokens
    <br /> - how to use Python to represent and visualize text data, with help from some libraries!
   
  </div>
  
  <div> First of all, don't forget to confirm your attendence on <a href="https://www.arts.ac.uk/study-at-ual/course-regulations/attendance-policy/attendance-monitoring"> SEAtS App!  </a></div>


   <div>ğŸ˜how to use numbers to represent text?
   <br /> -  today we'll see some fun text representation (e.g. word2vec) that could reveal fun relation between words like this:
     <br /> - vec("Paris") - vec("France") + vec("Germany") â‰ˆ vec("Berlin")
   </div>
  
  <div>
  <h3>ğŸ“Text Data</h3>
  <ul>
    <li>Text is one of the most common forms of human communication.</li>
    <li>It can be reviews, documents, messages, articles, and chat logs, etc. </li>
    <li>Unlike images in pixels, text does not have numeric structures â€” requires processing before analysis.</li>
    <li>It is the foundation of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> and modern AI assistants.</li>
  </ul>
</div>

<div>
  <h3>ğŸ˜¶â€ğŸŒ«ï¸Challenges of Text Data</h3>
  <ul>
    <li>Ambiguity (same words â†’ different meanings, different words â†’ same meanings)</li>
    <li>Sparsity (a lot of zeroes "wasted" in one-hot vectors)</li>
    <li>Variability and diversity (Different languages, slang, grammar)</li>
    <li>No inherent numeric structure (how to use numbers to represent text?) </li>
  </ul>
</div>
<div>
  <h3>ğŸ˜Basic Representation: Tokens</h3>
  <ul>
    <li>Processing a long paragraph/sentence, which is usually one long string variable in our computers, is made easier by firstly breaking it into smaller pieces.</li>
  </ul>
</div>
  
<div>
  <h3>ğŸ˜Basic Representation: Tokens</h3>
  <ul>
  <li>Tokenization breaks text into smaller and "reuseble" units.</li>
    <li>Tokenization could of at different levels - words as tokens, subwords as tokens, characters as tokens.</li>
    <li>For example, "i ate an apple this morning" can be tokenized into: ["i", "ate", "an", "apple", "this", "morning"] (from the word-level tokenization).</li>    
    <li>Python libraries: <a href="https://www.nltk.org/">NLTK</a>, <a href="https://spacy.io/">spaCy</a>, <a href="https://radimrehurek.com/gensim/">gensim</a>, <a href="https://github.com/openai/tiktoken">OpenAI tiktoken</a>.</li>
  </ul>
</div>

<div>ğŸ˜If we can find a way to represent these tokens numerically, we can then "numberify" any text!</div>

<div>âœ‹Next: let's try some hands-on code examples for tokenization
  <ul>
  <li>Let's prepare a folder for this week's coding example!
  <li>Let's open VSCode!
  <li>Let's ceate a new python notebook in the folder!
  <li>Let's run the notebook in VSCode!
  <li>Let's re-use last week's python environment by selecting it! (or <a href="https://red-x-silver.github.io/BSc-DSAI-Y1-DataRepresentationVisualization-2025-2026/week-06/slides.html#19">create a new environment</a> if you don't have one yet:)
  </ul>
</div>
  
<div>âœ‹Next: let's try some hands-on code examples for tokenization
  <ul>
    <li>Let's pip install the libraries we are going to use today: 
      <pre><code>
   !pip install nltk spacy wordcloud scikit-learn networkx matplotlib
  </code></pre>
    <li> (the use of "!" above is recommended, but not mandentory if you put just the "pip install" (without any other python code) in a code cell.)
  </ul>
</div>

  <div>
  <h3>âœ‹Demo: Tokenization (NLTK)</h3>
  <pre><code>
import nltk
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize

text = "Text data is everywhere!" # this is an example text
tokens = word_tokenize(text)
print(tokens)
  </code></pre>

  <p>Tokenization is the first step in almost all text workflows.</p>
</div>

<div>
  <h3>Common text preprocessing steps</h3>
  <p>Usually there are some other steps before converting tokens into numbers.</p>
  <ul>
    <li><a href="https://codesignal.com/learn/courses/text-data-preprocessing-in-python/lessons/lowercasing-text-for-uniformity-in-nlp">Lowercasing</a></li>
    <li><a href="https://codesignal.com/learn/courses/text-data-preprocessing-in-python/lessons/punctuating-punctuation-streamlining-text-for-nlp">Removing punctuation</a></li>
    <li><a href="https://codesignal.com/learn/courses/text-data-preprocessing-in-python/lessons/demystifying-stop-words-in-natural-language-processing">Stopword removal</a></li>
    <li><a href="https://codesignal.com/learn/courses/text-data-preprocessing-in-python/lessons/mastering-stemming-in-nlp-with-nltk">Stemming and Lemmatization</a></li>
  </ul>
  <p>note: you can also apply lowercasing and punctuation removal before tokenization!</p>
</div>
  
<div>
  <h3>âœ‹Demo: Preprocessing with spaCy</h3>
  <p>In a separated code cell, run:
    <br /> !python -m spacy download en_core_web_sm</p>
  <pre><code>

import spacy
nlp = spacy.load("en_core_web_sm")

doc = nlp("Cats are running and jumped over fences.")
tokens = [(token.text, token.lemma_) for token in doc]
print(tokens)
  </code></pre>

  <p>Lemma â†’ base form of words (running â†’ run).</p>
</div>

  <div>ğŸ˜Now we have the nice pre-processed and tokenized text, let's find out how to represent text numerically!</div>
  
<div>
  <h3>Several ways to represent text numerically</h3>
  <ul>
    <li><a href="https://medium.com/analytics-vidhya/one-hot-encoding-of-text-data-in-natural-language-processing-2242fefb2148">One-hot vectors</a></li>
    <li><a href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag-of-Words (BoW)</a></li>
    <li><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TFâ€“IDF</a> (term frequencyâ€“inverse document frequency)</li>
    <li><a href="https://en.wikipedia.org/wiki/Word_embedding">Word embeddings</a> (Word2Vec, GloVe)</li>
    <li><a href="">Transformer-based embeddings (more on this in future AI units)</a></li>
  </ul>
</div>
<div>
  <h3><a href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag-of-Words (BoW)</a></h3>
  <ul>
    <li>Counts word occurrences in a document.</li>
    <li>Simple, interpretable, sparse.</li>
    <li>No order or grammar information.</li>
  </ul>
</div>
<div>
  <h3>âœ‹Demo: Bag-of-Words (sklearn)</h3>
  <pre><code>
from sklearn.feature_extraction.text import CountVectorizer

docs = ["cats drink milk", "dogs drink water"]
vec = CountVectorizer()
X = vec.fit_transform(docs)

print(vec.get_feature_names_out())
print(X.toarray())
  </code></pre>
</div>
  
<div>
  <h3><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TFâ€“IDF</a></h3>
  <p>Weights words by importance, not just frequency.</p>
  <ul>
    <li>TF â†’ how often a word appears</li>
    <li>IDF â†’ how rare it is across documents</li>
    <li>Downweights common words (â€œtheâ€, â€œofâ€, â€œandâ€)</li>
  </ul>
</div>
<div>
  <h3>âœ‹Demo: TFâ€“IDF</h3>
  <pre><code>
from sklearn.feature_extraction.text import TfidfVectorizer

docs = ["machine learning is fun", "deep learning is interesting"]
tfidf = TfidfVectorizer()
X = tfidf.fit_transform(docs)

print(tfidf.get_feature_names_out())
print(X.toarray())
  </code></pre>
</div>
  
<div>
  <h3><a href="https://en.wikipedia.org/wiki/Word_embedding">Word embeddings</a></h3>
  <p>Dense vector representations where:</p>
  <ul>
    <li>Similar words have similar vectors.</li>
    <li>Capture semantic meaning.</li>
    <li>Examples: <a href="https://jalammar.github.io/illustrated-word2vec/">Word2Vec</a>, GloVe, FastText.</li>
  </ul>
</div>

    <div>ğŸ˜fun equations from common word2vec embeddings:
   <br />  vec("brother") - vec("man") + vec("woman") â‰ˆ vec("sister")
     <br />  vec("Paris") - vec("France") + vec("Germany") â‰ˆ vec("Berlin")
   </div>
  

  <div>
 Let's use a realistic text dataset for demonstrating word2vec text representation!
 <br /> - the dataset: <a href="https://www.kaggle.com/datasets/quora/question-pairs-dataset/">Question Pairs Dataset</a> 
<br /> - this is a csv file, put it into today's working folder (where our python notebook is) and let's explore it briefly!
</div> 
 
  
<div>
  <h3>âœ‹ Demo: word2vec</h3>
  <p>Useful for showing clusters of similar words.</p>
 <p>let's use some help from a library called "genism": !pip install gensim</p>
  <p> - downlaod the prepared notebook <a href="https://moodle.arts.ac.uk/mod/resource/view.php?id=1566884">here</a>, copy the cells under "Chapter 2" into your notebook and run them!</p>

</div>

  <div>
 Note: word2vec is a worth-exploring topic, which also requires a little bit more deep learning understanding.
 <br />  <a href="https://remykarem.github.io/word2vec-demo/">Here</a> is a web demo for word2vec and t-sne visualization training, with recommended learning resources listed in the "reference" section.
</div>  
  
  
<div>
  <h3>âœ‹Demo: Word Cloud Visualization</h3>
  <p>Displays most frequent words based on size.</p>
   <p>Let's take a look at the cell from the prepared notebook!</p>
  
</div>
<div>
  <h3>Co-occurrence Graphs</h3>
  <ul>
    <li>Visualize which words appear together.</li>
    <li>Great for topic exploration.</li>
  </ul>
</div>
  
<div>
  <h3>âœ‹Demo: Co-occurrence Graph</h3>
  <p>Let's take a look at the cell from the prepared notebook!</p>
</div>
  
<div>
  <h3>[optional] Classical embedding â†’ Modern transformer embeddings</h3>
   <p>Text representations from a more advanced deep learning technique.</p>
  <ul>
    <li>Before: BoW, TFâ€“IDF, Word2Vec</li>
    <li>Now: Transformer embeddings (<a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a>, GPT)</li>
    <li>Meaning is encoded contextually, not statically.</li>
  </ul>
</div>
<div>
  <h3>[optional]How Transformers Represent Text</h3>
  <ul>
    <li>Tokenization â†’ subword units (byte-pair encoding)</li>
    <li>Vector embedding for each token</li>
    <li>Self-attention captures relationships across tokens</li>
    <li>Final output = contextual embeddings</li>
  </ul>
</div>
  
<div>
  <h3>ğŸ•¶ï¸ What we have learnt today:</h3>
  <ul>
    <li>The pipeline for representing any piece of text using numbers: pre-processing -> tokenization -> BoW/TF-IDF/Word Embeddings </li>
    <li>Classic representations: BoW, TFâ€“IDF.</li>
    <li>Modern representations: embeddings like word2vec, transformers.</li>
    <li>Text visualization methods: word cloud, co-occurrence graph</li>
    <li>âœ‹Hands-on practice on representing and visualizing text data with help from Python libraries: <a href="https://www.nltk.org/">NLTK</a>, <a href="https://spacy.io/">spaCy</a>, <a href="https://radimrehurek.com/gensim/">gensim</a></li>
    <li>âœ‹Hands-on practice on how to train a word2vec embedding on a text dataset, and compute word similarity based on the trained word2vec</li>
  </ul>
</div>

 

  <div> 
     Homework:
   <br />
    - Start preparing for your <a href="https://moodle.arts.ac.uk/mod/resource/view.php?id=1559494">final project</a>! 
  <br />
    - If you plan to work with content we have covered so far, send me a small proposal on what you plan to make and I'll recommend relevant resources!
  <br />
    - If you plan to work with content we have not covered so far, send me a message and I'll recommend early readings/preparation for you!
  </div>
  </div>
  
  <div> 
    We'll see you next Tuesday same time and same place!
  </div>
  
</body>
</html>

  
  
